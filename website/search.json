[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Analysis of Educational Diagnostic Questions",
    "section": "",
    "text": "Abstract\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295x.111.4.1061.\n\n\nCronbach, L. J., and P. E. Meehl. 1966. “Construct Validity in Psychological Tests.” In Readings in Clinical Psychology, 29–52. Elsevier. https://doi.org/10.1016/b978-1-4832-0087-3.50007-3.\n\n\nCronbach, Lee J. 1942. “An Analysis of Techniques for Diagnostic Vocabulary Testing.” The Journal of Educational Research 36 (3): 206–17. http://www.jstor.org/stable/27528353.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nLuan, Hui, and Chin-Chung Tsai. 2021. “A Review of Using Machine Learning Approaches for Precision Education.” Educational Technology & Society 24 (1): 250–66. https://www.jstor.org/stable/26977871.\n\n\nMurphy, Robert F. 2019. “Artificial Intelligence Applications to Support k–12 Teachers and Teaching: A Review of Promising Applications, Opportunities, and Challenges.” RAND Corporation. http://www.jstor.org/stable/resrep19907.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nSpector, J. Michael, Dirk Ifenthaler, Demetrios Sampson, Lan (Joy) Yang, Evode Mukama, Amali Warusavitarana, Kulari Lokuge Dona, et al. 2016. “Technology Enhanced Formative Assessment for 21st Century Learning.” Journal of Educational Technology & Society 19 (3): 58–71. http://www.jstor.org/stable/jeductechsoci.19.3.58.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E. Turner, et al. 2021. “Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.\n\n\nYang, Christopher C. Y., Irene Y. L. Chen, and Hiroaki Ogata. 2021. “Toward Precision Education: Educational Data Mining and Learning Analytics for Identifying Students’ Learning Patterns with Ebook Systems.” Educational Technology & Society 24 (1): 152–63. https://www.jstor.org/stable/26977864.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "intro.html#background",
    "href": "intro.html#background",
    "title": "1  Introduction",
    "section": "1.1 Background",
    "text": "1.1 Background\nIn education, diagnostic testing plays a significant role in helping educators understand the needs and capabilities of students. It allows educators to tailor their teaching strategies to fit the unique circumstances of different learners, and it assists them in identifying areas in which students may need special interventions (Lee J. Cronbach 1942). Failure to draw attention to students’ weaknesses as early and accurately as possible can have consequences that reach far beyond the context in which a diagnostic test is administered. Consequently, the questions of how to devise diagnostic testing items that are responsive to students’ abilities and how to best interpret diagnostic data that has already been collected are not trivial.\nIn the context of educational measurement and assessment, the validity of a test is dependent on the degree to which it actually measures what it claims to measure (Borsboom, Mellenbergh, and Heerden 2004). A test is only valid if it is sensitive to varying levels of some latent trait that causally produces the outcomes observed in test scores. If one can show that performance on a test is likely driven by variance in some attribute that is irrelevant to the construct that the test purports to measure, then the validity of the test is threatened. The importance of test validity in education reaches far beyond any particular testing instrument. In a broader sense, validity can be understood as referring to the extent to which evidence supports the interpretations and suggested uses of test scores (L. J. Cronbach and Meehl 1966).\nA necessary condition for the reasonable use of any test is a convincing body of evidence for the validity of a particular interpretation of test scores. In this regard, there are several ways that validity evidence can be weak or otherwise insufficient (Borsboom, Mellenbergh, and Heerden 2004). For instance, a test might fail to capture some relevant aspects of the construct it was designed to measure. A test might also be exceedingly susceptible to the influence of processes that are entirely unrelated to its intended purpose. For example, a reading comprehension test would be subject to accusations of construct-irrelevance if the content of the test elicited an emotional reaction that interfered with the test takers performance. There are numerous other threats to test validity, but what they have in common is that they call into question the claims and decisions that are made based on test results. The question of validity is especially relevant in diagnostic testing, the purpose of which is to inform decisions about the path that a student’s instructional plan should follow. This is the topic with which the NeurIPS 2020 Education Challenge was concerned (Wang, Lamb, Saveliev, Cameron, Zaykov, Hernández-Lobato, et al. 2021). In an effort to improve the technologies that are used to develop individualized learning resources for students, the competition tasked participants with devising new methods to understand students’ response patterns and assess the quality of diagnostic questions. The challenge utilized data from an online education service provider called Eedi consisting of tens of thousands of multiple-choice questions that were administered over two years to a range of students from elementary to high-school grades. The test items were all targeted to different skills in mathematics, including Algebra, Geometry, and Statistics. The matter of validity is immediately relevant to the challenge. In order for the instructional decision making that results from the administration of diagnostic tests to be sound, the tests themselves must be valid. In order to be valid, the diagnostic test questions need to capture variation in respondents’ mathematical abilities. The diagnostic items can be considered valid if there is evidence that they do in fact measure what the test administrators claim that they measure.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#research-questions",
    "href": "intro.html#research-questions",
    "title": "1  Introduction",
    "section": "1.2 Research Questions",
    "text": "1.2 Research Questions\nThis analysis intends to test the validity of the test items used in the challenge and look for construct-irrelevant factors that may have influenced the responses to test items. We take up the following research questions:\n\nDo the item response data provide evidence to support the subject domains present in the test?\nCan relatively accessible machine learning models predict student performance with enough precision to inform educational decision making?\n\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295x.111.4.1061.\n\n\nCronbach, L. J., and P. E. Meehl. 1966. “Construct Validity in Psychological Tests.” In Readings in Clinical Psychology, 29–52. Elsevier. https://doi.org/10.1016/b978-1-4832-0087-3.50007-3.\n\n\nCronbach, Lee J. 1942. “An Analysis of Techniques for Diagnostic Vocabulary Testing.” The Journal of Educational Research 36 (3): 206–17. http://www.jstor.org/stable/27528353.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nLuan, Hui, and Chin-Chung Tsai. 2021. “A Review of Using Machine Learning Approaches for Precision Education.” Educational Technology & Society 24 (1): 250–66. https://www.jstor.org/stable/26977871.\n\n\nMurphy, Robert F. 2019. “Artificial Intelligence Applications to Support k–12 Teachers and Teaching: A Review of Promising Applications, Opportunities, and Challenges.” RAND Corporation. http://www.jstor.org/stable/resrep19907.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nSpector, J. Michael, Dirk Ifenthaler, Demetrios Sampson, Lan (Joy) Yang, Evode Mukama, Amali Warusavitarana, Kulari Lokuge Dona, et al. 2016. “Technology Enhanced Formative Assessment for 21st Century Learning.” Journal of Educational Technology & Society 19 (3): 58–71. http://www.jstor.org/stable/jeductechsoci.19.3.58.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E. Turner, et al. 2021. “Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.\n\n\nYang, Christopher C. Y., Irene Y. L. Chen, and Hiroaki Ogata. 2021. “Toward Precision Education: Educational Data Mining and Learning Analytics for Identifying Students’ Learning Patterns with Ebook Systems.” Educational Technology & Society 24 (1): 152–63. https://www.jstor.org/stable/26977864.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html#data",
    "href": "methods.html#data",
    "title": "2  Methods",
    "section": "2.1 Data",
    "text": "2.1 Data\nThe data used is from the NeurIPS 2020 Education Challenge (Wang et al. 2020), which is in the format of question-answer pairs of mathematical questions posed to students and their answers (and demographic). There are more than a 200 million data points in the full dataset, so we use a subset of about only a million data points. We join across the multiple tables that the data is present in, and combine them in order to have full information for each data point.\nThe data used format can be seen in Table 2.1. We dropped unused columns (CorrectAnswer, AnswerValue, SchemeOfWorkId), and the transformations for feature engineered columns are listed in the descriptions. The description of the columns used are as follows:\n\nQuestionId: ID of the question answered. Numeric.\nUserId: ID of the student who answered the question. Numeric.\nAnswerId: Unique identifier for the (QuestionId, UserId) pair, used to join with associated answer metadata (see below). Numeric.\nIsCorrect: Binary indicator for whether the student’s answer was correct (1 is correct, 0 is incorrect). Categorical.\nSubjectId: Each subject covers an area of mathematics, at varying degrees of gran- ularity. We provide IDs for each topic associated with a question in a list. Example topics could include “Algebra”, “Data and Statistics”, and “Geometry and Measure”. These subjects are arranged in a tree structure, so that for instance “Factorising” is the parent subject of “Factorising into a Single Bracket”. We provide details of this tree in an additional file subject metadata.csv which contains the subject name and tree level associated with each SubjectId, in addition to the SubjectId of its parent subject. Categorical.\nCategory1: Feature engineered. The first-level category of the question (given that there is hierarchical categories). Categorical.\nGender: The student’s gender, when available. 0 is unspecified, 1 is female, 2 is male and 3 is other. Categorical.\nAge: Feature engineered. The student’s age, as calculated from DateAnswered - DateOfBirth. Numeric.\nPremiumPupil: Whether the student is eligible for free school meals or pupil premium due to being financially disadvantaged. Categorical.\nDateAnswered: Time and date that the question was answered, to the nearest minute. Time sequence/numeric.\nConfidence: Percentage confidence score given for the answer. 0 means a random guess, 100 means total confidence. Numeric.\nGroupId: The class (group of students) in which the student was assigned the question. Categorical.\nQuizId: The assigned quiz which contains the question the student answered. Categorical.\n\n\n\n\n\nTable 2.1: The source data used in this paper, tranformed by unions across several csvs, some columns dropped, and some created columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestionId\nUserId\nAnswerId\nIsCorrect\nDateAnswered\nConfidence\nGroupId\n\n\n\n\n0\n898\n2111\n280203\n1\n2019-12-08 17:47:00\nnan\n95\n\n\n1\n767\n3062\n55638\n1\n2019-10-27 20:54:00\n25\n115\n\n\n2\n165\n1156\n386475\n1\n2019-10-06 20:16:00\nnan\n101\n\n\n3\n490\n1653\n997498\n1\n2020-02-27 17:40:00\nnan\n46\n\n\n4\n298\n3912\n578636\n1\n2019-12-27 16:07:00\nnan\n314\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuizId\nGender\nPremiumPupil\nSubjectId\nAge\nCategory1\n\n\n\n\n0\n86\n2\nFalse\n[3, 49, 62, 70]\n12\nAlgebra\n\n\n1\n39\n0\nFalse\n[3, 32, 144, 204]\nnan\nNumber\n\n\n2\n39\n0\nFalse\n[3, 32, 37, 220]\nnan\nNumber\n\n\n3\n115\n0\nFalse\n[3, 49, 81, 406]\nnan\nAlgebra\n\n\n4\n78\n2\nFalse\n[3, 71, 74, 180]\n11\nGeometry and Measure\n\n\n\n\n\n\n\n\n\n2.1.1 Exploratory Analysis\nWe perform initial EDA, which can be seen in Section 3 (Results). The EDA performed, in order to gain insight into the data, is as follows:\n\nSummary statistics of each column used.\nA sunburst plot of all the subject categories found in the question dataset.\nA histogram of the proportion of questions answered correctly by each student.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#statistical-methods-analysis",
    "href": "methods.html#statistical-methods-analysis",
    "title": "2  Methods",
    "section": "2.2 Statistical Methods Analysis",
    "text": "2.2 Statistical Methods Analysis\n\n2.2.1 Clustering\nWe perform clustering in order to answer the question: are the distances/similarity coefficients between the factors of the dataset, indicative of the subject category ID given to each data point? More specifically, when the columns given above are clustered, do we achieve a clustering similar to the labels of Category1 given to each data point? In order to achieve this, we must take into account the categorical factors in the dataset (such as QuizId and Gender) and the fact that we cannot simply compute minimal Euclidian distances between the values, even when ordinal. We utilize 2 different methods for handling categorical factors when clustering.\n\n2.2.1.1 Using the kmodes library\nWe use the kprototypes algorithm from the kmodes library. k-modes is used for clustering categorical variables. It defines clusters based on the number of matching categories between data points. (This is in contrast to the more well-known k-means algorithm, which clusters numerical data based on Euclidean distance.) The k-prototypes algorithm combines k-modes and k-means and is able to cluster mixed numerical / categorical data (Vos 2015--2021).\n\n\n2.2.1.2 Using the Gower Distance\nGower’s Distance can be used to measure how different two records are (Gower 1971). The records may contain combination of logical, categorical, numerical or text data. The distance is always a number between 0 (identical) and 1 (maximally dissimilar). The metrics used for each data type are described below:\n\nquantitative (interval): range-normalized Manhattan distance\nordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties\nnominal: variables of k categories are first converted into k binary columns and then the Dice coefficient is used\n\nThis distance metric can be used to calculate a distance matric between all points in the dataset, which can then be used by standard hierarchical clustering. We use the scikit-learn package with its Agglomerative clustering algorithm, and cluster across multiple linkage types (as different types of linkage can produce vastly different clusters) (Pedregosa et al. 2011).\n\n\n2.2.1.3 Performance metrics\nIn order to measure how well the clustering results approximate the question category labels given, we use the Rand index for similarity. It is a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering (Rand 1971). A value of 0 indicates no similarity (clusterings do not agree on any pair of points), and 1 indicates perfect matching in clustering labels. A form of the Rand index, called the adjusted Rand index, is adjusted for the chance grouping of elements.\n\n\n\n2.2.2 Supervised Learning Models\nWe also run various supervised learning models on the data, in order to answer the following question: can non-deep learning (aka not neural network) models learn, based on the given factors in the data, whether a student will answer a question correctly? Specicifally, we run models on a transformed version of the dataset in order to predict the label column of IsCorrect. The transformations performed on the columns of the dataset are:\n\nNumerical values were min-max normalized to 0-1.\nThe timeseries column (DateAnswered) was transformed into an integer.\nThe categorical columns were one-hot encoded, where each category in each factor recieves its own column of 0-1 values (with 1 indicating that value is present), essentially creating a sparse matrix subset.\n\nWe run 7 different models on the dataset, with all implemented in scikit-learn (Pedregosa et al. 2011). The models used are as follows:\n\nLogistic Regression: A simple logistic regression classifier, where parameters (for each factor plus bias/intercept) are fitted to a linear model.\nLogistic Regression with Stochastic Gradient Descent (SGD): SGD improves on gradient descent by replacing the gradient with an esimation of it, reducing computational complexity.\nPerceptron: A linear predictor which uses a set of weights with the feature vector to output a binary clasisfier.\nLinear Support Vector Machine: Maps training examples to points in space so as to maximise the width of the gap between the two categories. Used to perform linear classification.\nDecision Tree: A model where decisions are represented by a tree/flowchart structure, each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules (Wikipedia contributors 2023).\nRandom Forest Classifier: A meta-model where a number of decision trees are fitted on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting (Pedregosa et al. 2011).\nHistogram-based Gradient Boosting Classification Tree: A meta-model that fits multiple gradient-boosted decision tree classifiers, that has support for NaN values, categorical values, and is computionally quick dues to binning inputs into histograms instead of naive evaluation.\n\n\n2.2.2.1 Performance metrics\nTo evaluate the models, we use standard performance metrics that are used for supervised learning on binary classification. The metrics we display are:\n\nAccuracy: The ratio of correct predictions to all predictions. In other words, the total of the green squares in a confusion matrix divided by the entire matrix. This is arguably the most common concept of measuring performance. It ranges from 0-1 with 1 being the best performance.\nPrecision: The ratio of true positives to the total number of positives (true positive + true negative).\nRecall: The ratio of true positives to the number of total correct predictions (true positive + false negative).\nF1 Score: Known as the harmonic mean between precision and recall. Precision and Recall are useful in their own rights, but the F1-Score is useful in the fact it’s a balanced combination of both precision and recall. It ranges from 0-1 with 1 being the best performance.\nSupport: The number of true instances for each label.\n\nIn addition, we use several visualizations to display perfomance of the models:\n\nROC Curve: A plot of the true positive rate vs the false positive rate, as a curve. We examine the AUC (area under the curve) to determine how well that a randomly chosen positive example is indeed labeled positive. If it follows the straight diagonal line, the AUC is low and therefore the classifier is no better than chance. If there’s a high AUC, then the classifier is performing well. The baseline AUC is 0.5, a a perfect classififer has 1.0\nConfusion Matrix: A matrix showing the amount of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\nPrecision-Recall Curve: A model can improve in precision or recall, but not both. A PR curve shows that tradeoff, and how well it performs in both. The curve is constructed by calculating and plotting the precision against the recall for a single classifier at a variety of thresholds. A perfect classifier would have a line that starts high and straight, and curves down only near the end of the recall axis. The summary value for the curve is the AP, or average precision; higher values towards 1 are better.\n\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295x.111.4.1061.\n\n\nCronbach, L. J., and P. E. Meehl. 1966. “Construct Validity in Psychological Tests.” In Readings in Clinical Psychology, 29–52. Elsevier. https://doi.org/10.1016/b978-1-4832-0087-3.50007-3.\n\n\nCronbach, Lee J. 1942. “An Analysis of Techniques for Diagnostic Vocabulary Testing.” The Journal of Educational Research 36 (3): 206–17. http://www.jstor.org/stable/27528353.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nLuan, Hui, and Chin-Chung Tsai. 2021. “A Review of Using Machine Learning Approaches for Precision Education.” Educational Technology & Society 24 (1): 250–66. https://www.jstor.org/stable/26977871.\n\n\nMurphy, Robert F. 2019. “Artificial Intelligence Applications to Support k–12 Teachers and Teaching: A Review of Promising Applications, Opportunities, and Challenges.” RAND Corporation. http://www.jstor.org/stable/resrep19907.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nSpector, J. Michael, Dirk Ifenthaler, Demetrios Sampson, Lan (Joy) Yang, Evode Mukama, Amali Warusavitarana, Kulari Lokuge Dona, et al. 2016. “Technology Enhanced Formative Assessment for 21st Century Learning.” Journal of Educational Technology & Society 19 (3): 58–71. http://www.jstor.org/stable/jeductechsoci.19.3.58.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E. Turner, et al. 2021. “Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.\n\n\nYang, Christopher C. Y., Irene Y. L. Chen, and Hiroaki Ogata. 2021. “Toward Precision Education: Educational Data Mining and Learning Analytics for Identifying Students’ Learning Patterns with Ebook Systems.” Educational Technology & Society 24 (1): 152–63. https://www.jstor.org/stable/26977864.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "results.html#clustering",
    "href": "results.html#clustering",
    "title": "3  Results",
    "section": "3.1 Clustering",
    "text": "3.1 Clustering\nClustering was performed as described in Section 2 (Methods); the kmodes library was used on the base dataset, the Gower distances matrix was computed in order to handle categorical variables, and the distance matrix was used with scikit-learn library for Agglomerative clustering. In addition, for the Agglomerative clustering, 3 different linkages between distances were used: single, average, and complete.\n\n3.1.1 Perfomance\nIn order to judge the clustering output, the Rand index was calculated as described in the previous section. We compute the Rand index, and the adjusted Rand index, not only between the source labels and the computed clusters but also between each clustering method. The rand index results are displayed in Figure 3.3, as a heat map matrix between all clustering types and the source labels. The highest Rand index with the source category labels was the Kprototypes clustering algorithm, with a value of 0.66.\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Rand Index Matrix\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Adjusted Rand Index\n\n\n\n\n\n\n\nFigure 3.3: Matrices of the rand index as compared across all clustering methods, and compared to the original source category labels. The scipy and sklearn clustering methods were done using a precomputed Gower matrix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#supervised-learning-models-performance-metrics",
    "href": "results.html#supervised-learning-models-performance-metrics",
    "title": "3  Results",
    "section": "3.2 Supervised Learning Models Performance Metrics",
    "text": "3.2 Supervised Learning Models Performance Metrics\nThe supervised learning models were run as described in the previous section, with their default values used for most parameters (such as loss function). For most models, the one modified parameter was the number of iterations run in order to train the model, by increasing it to 1000.\nFor the best performing model (HistGradientBoostingClassifier), we made the following modifications in an attempt to improve its performance even further:\n\nThe data was not transformed from the original source columns, leaving the categories nad numbers unchanged (with the exception of DateAnswered, which was still converted to an int).\nThe maximum number of leaf nodes was increased to 80.\nThe category factors were given as categorical input.\n\nThis resulted in the new best performing model, HistGradientBoostingClassifier2. The summary and performance statistics for each model are summarized in Table 3.2, with the metrics as described in the previous section.\n\n\n\n\nTable 3.2: The performance values and some settings for the supervized learning models ran, sorted by accuracy.\n\n\n\n\n\n\n\n\nModel\nAccuracy\nprecision\nrecall\nf1-score\nsupport\nTrainIters\nLossFcn\n\n\n\n\nHistGradientBoostingClassifier2\n0.71441\n0.71472\n0.71441\n0.71454\n138273.00000\n\nlog_loss\n\n\nHistGradientBoostingClassifier\n0.66020\n0.65997\n0.66020\n0.66007\n138273.00000\n\nlog_loss\n\n\nRandomForestClassifier\n0.65193\n0.65193\n0.65193\n0.65193\n138273.00000\n\n\n\n\nDecisionTreeClassifier\n0.61077\n0.61074\n0.61077\n0.61075\n138273.00000\n\n\n\n\nLinearSVC\n0.57034\n0.56629\n0.57034\n0.55989\n138273.00000\n12\nsquared_hinge\n\n\nLogisticRegression\n0.57028\n0.56623\n0.57028\n0.56008\n138273.00000\n[0]\n\n\n\nSGDClassifier\n0.56954\n0.56555\n0.56954\n0.55500\n138273.00000\n41\nlog_loss\n\n\nPerceptron\n0.47191\n0.45654\n0.47191\n0.45497\n138273.00000\n11\nperceptron\n\n\n\n\n\n\n\n\n\n3.2.1 Performance Plots\nWe display the ROC curve, confusion matrix, and precision-recall curves for the worst (Perceptron) and best (HistGradientBoostingClassifier2) performing models in Figure 3.4. We observe that we only see “typical” ROC and PR curves for the best performing model.\n\n\n\n\n\n\n\n\n\n\n\n(a) Perceptron: ROC Curve\n\n\n\n\n\n\n\n\n\n\n\n(b) HistGradientBoostingClassifier: ROC Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Perceptron: confusion matrix\n\n\n\n\n\n\n\n\n\n\n\n(d) HistGradientBoostingClassifier: confusion matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Perceptron: precison-recall curve\n\n\n\n\n\n\n\n\n\n\n\n(f) HistGradientBoostingClassifier: precison-recall curve\n\n\n\n\n\n\n\nFigure 3.4: The ROC, confusion matrix, and precision-recall curves for the best and worst performing model.\n\n\n\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295x.111.4.1061.\n\n\nCronbach, L. J., and P. E. Meehl. 1966. “Construct Validity in Psychological Tests.” In Readings in Clinical Psychology, 29–52. Elsevier. https://doi.org/10.1016/b978-1-4832-0087-3.50007-3.\n\n\nCronbach, Lee J. 1942. “An Analysis of Techniques for Diagnostic Vocabulary Testing.” The Journal of Educational Research 36 (3): 206–17. http://www.jstor.org/stable/27528353.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nLuan, Hui, and Chin-Chung Tsai. 2021. “A Review of Using Machine Learning Approaches for Precision Education.” Educational Technology & Society 24 (1): 250–66. https://www.jstor.org/stable/26977871.\n\n\nMurphy, Robert F. 2019. “Artificial Intelligence Applications to Support k–12 Teachers and Teaching: A Review of Promising Applications, Opportunities, and Challenges.” RAND Corporation. http://www.jstor.org/stable/resrep19907.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nSpector, J. Michael, Dirk Ifenthaler, Demetrios Sampson, Lan (Joy) Yang, Evode Mukama, Amali Warusavitarana, Kulari Lokuge Dona, et al. 2016. “Technology Enhanced Formative Assessment for 21st Century Learning.” Journal of Educational Technology & Society 19 (3): 58–71. http://www.jstor.org/stable/jeductechsoci.19.3.58.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E. Turner, et al. 2021. “Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.\n\n\nYang, Christopher C. Y., Irene Y. L. Chen, and Hiroaki Ogata. 2021. “Toward Precision Education: Educational Data Mining and Learning Analytics for Identifying Students’ Learning Patterns with Ebook Systems.” Educational Technology & Society 24 (1): 152–63. https://www.jstor.org/stable/26977864.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "4  Conclusions",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#findings",
    "href": "conclusions.html#findings",
    "title": "4  Conclusions",
    "section": "4.1 Findings",
    "text": "4.1 Findings\nWhile the analysis conducted here did not produce any evidence to threaten the validity of the diagnostic test items in question and did not provide any reason to conclude that construct-irrelevant factors were driving student responses, we feel that there is reason to be optimistic about the relative success of the machine learning models. To the extent that the models were successful in generating information that could be used to guide educational decision making, they provide evidence in favor of using such models in other tasks such as machine grading, precision educational interventions, and automated tutoring. Machine grading is one area in which accurate and efficient models could ease the burden on educators by limiting the time they need to spend trying to provide meaningful and actionable feedback to students (Spector et al. 2016).\nWith regard to precision educational interventions, the applications of effective machine learning models span a broad spectrum of contexts. For example, existing research points to potential uses for predicting dropouts and disciplinary problems, as well as other matters related to attrition and retention (Luan and Tsai 2021). So, even though this analysis focused on an application for predicting response patterns to mathematics questions, there is no reason that the same models could not be used for prediction and diagnosis in other areas of education. This is especially relevant in the aftermath of the massive interruptions to students’ education during and after the pandemic when educators across the country are racing to find ways to remedy the severe learning loss that took place.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#limitations",
    "href": "conclusions.html#limitations",
    "title": "4  Conclusions",
    "section": "4.2 Limitations",
    "text": "4.2 Limitations\nOne limitation of this study is that there was relatively little student demographic information available. If a richer set of variables had been available in the data set, then it is at least conceivable that construct-irrelevance or differential item functioning might have emerged from the data. There was also no information about student response times, which is another indicator that is commonly used to investigate whether certain families of items operate in different ways among different groups of respondents. A third limitation is that very little is known about the students who are present in the sample. Fairly simple information about how much exposure students have had to certain kinds of questions and what kind of instruction they receive was unavailable. Any of these limitations on their own or in tandem could make for very different results with regard to test validity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#future-studies",
    "href": "conclusions.html#future-studies",
    "title": "4  Conclusions",
    "section": "4.3 Future Studies",
    "text": "4.3 Future Studies\nFuture iterations of this kind of work might attempt to focus the predictive abilities of the machine learning models to behavioral and social-emotional characteristics of students based on school-level data that is typically recorded. For example, attendance, chronic absences, discipline referrals, and expulsions are all kinds of information that are routinely collected by schools. This kind of information combined with standard metrics of academic performance might lead to incredibly useful insights that allow teachers and administrators to identify and design interventions for the most at-risk students.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  }
]