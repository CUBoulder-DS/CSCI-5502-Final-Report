[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Analysis of Educational Diagnostic Questions",
    "section": "",
    "text": "Abstract\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "intro.html#background",
    "href": "intro.html#background",
    "title": "1  Introduction",
    "section": "1.1 Background",
    "text": "1.1 Background\n\nExample citations are (haradai1262 2020), and another (shshen-closer (2021)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#research-questions",
    "href": "intro.html#research-questions",
    "title": "1  Introduction",
    "section": "1.2 Research Questions",
    "text": "1.2 Research Questions\n\n\n1.2.1 Objective\n\n\n1.2.2 Rationale\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html#data",
    "href": "methods.html#data",
    "title": "2  Methods",
    "section": "2.1 Data",
    "text": "2.1 Data\nThe data used is from the NeurIPS 2020 Education Challenge (Wang et al. 2020), which is in the format of question-answer pairs of mathematical questions posed to students and their answers (and demographic). There are more than a 200 million data points in the full dataset, so we use a subset of about only a million data points. We join across the multiple tables that the data is present in, and combine them in order to have full information for each data point.\nThe data used format can be seen in Table 2.1. We dropped unused columns (CorrectAnswer, AnswerValue, SchemeOfWorkId), and the transformations for feature engineered columns are listed in the descriptions. The description of the columns used are as follows:\n\nQuestionId: ID of the question answered. Numeric.\nUserId: ID of the student who answered the question. Numeric.\nAnswerId: Unique identifier for the (QuestionId, UserId) pair, used to join with associated answer metadata (see below). Numeric.\nIsCorrect: Binary indicator for whether the student’s answer was correct (1 is correct, 0 is incorrect). Categorical.\nSubjectId: Each subject covers an area of mathematics, at varying degrees of gran- ularity. We provide IDs for each topic associated with a question in a list. Example topics could include “Algebra”, “Data and Statistics”, and “Geometry and Measure”. These subjects are arranged in a tree structure, so that for instance “Factorising” is the parent subject of “Factorising into a Single Bracket”. We provide details of this tree in an additional file subject metadata.csv which contains the subject name and tree level associated with each SubjectId, in addition to the SubjectId of its parent subject. Categorical.\nCategory1: Feature engineered. The first-level category of the question (given that there is hierarchical categories). Categorical.\nGender: The student’s gender, when available. 0 is unspecified, 1 is female, 2 is male and 3 is other. Categorical.\nAge: Feature engineered. The student’s age, as calculated from DateAnswered - DateOfBirth. Numeric.\nPremiumPupil: Whether the student is eligible for free school meals or pupil premium due to being financially disadvantaged. Categorical.\nDateAnswered: Time and date that the question was answered, to the nearest minute. Time sequence/numeric.\nConfidence: Percentage confidence score given for the answer. 0 means a random guess, 100 means total confidence. Numeric.\nGroupId: The class (group of students) in which the student was assigned the question. Categorical.\nQuizId: The assigned quiz which contains the question the student answered. Categorical.\n\n\n\n\n\nTable 2.1: The source data used in this paper, tranformed by unions across several csvs, some columns dropped, and some created columns.\n\n\n\n\n\n\n(a) First set of columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestionId\nUserId\nAnswerId\nIsCorrect\nDateAnswered\nConfidence\nGroupId\n\n\n\n\n0\n898\n2111\n280203\n1\n2019-12-08 17:47:00\nnan\n95\n\n\n1\n767\n3062\n55638\n1\n2019-10-27 20:54:00\n25\n115\n\n\n2\n165\n1156\n386475\n1\n2019-10-06 20:16:00\nnan\n101\n\n\n3\n490\n1653\n997498\n1\n2020-02-27 17:40:00\nnan\n46\n\n\n4\n298\n3912\n578636\n1\n2019-12-27 16:07:00\nnan\n314\n\n\n\n\n\n\n\n\n\n\n\n(b) Second set of columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuizId\nGender\nPremiumPupil\nSubjectId\nAge\nCategory1\n\n\n\n\n0\n86\n2\nFalse\n[3, 49, 62, 70]\n12\nAlgebra\n\n\n1\n39\n0\nFalse\n[3, 32, 144, 204]\nnan\nNumber\n\n\n2\n39\n0\nFalse\n[3, 32, 37, 220]\nnan\nNumber\n\n\n3\n115\n0\nFalse\n[3, 49, 81, 406]\nnan\nAlgebra\n\n\n4\n78\n2\nFalse\n[3, 71, 74, 180]\n11\nGeometry and Measure\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.1 Exploratory Analysis\nWe perform initial EDA, which can be seen in Section 3 (Results). The EDA performed, in order to gain insight into the data, is as follows:\n\nSummary statistics of each column used.\nA sunburst plot of all the subject categories found in the question dataset.\nA histogram of the proportion of questions answered correctly by each student.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#statistical-methods-analysis",
    "href": "methods.html#statistical-methods-analysis",
    "title": "2  Methods",
    "section": "2.2 Statistical Methods Analysis",
    "text": "2.2 Statistical Methods Analysis\n\n2.2.1 Clustering\nWe perform clustering in order to answer the question: are the distances/similarity coefficients between the factors of the dataset, indicative of the subject category ID given to each data point? More specifically, when the columns given above are clustered, do we achieve a clustering similar to the labels of Category1 given to each data point? In order to achieve this, we must take into account the categorical factors in the dataset (such as QuizId and Gender) and the fact that we cannot simply compute minimal Euclidian distances between the values, even when ordinal. We utilize 2 different methods for handling categorical factors when clustering.\n\n2.2.1.1 Using the kmodes library\nWe use the kprototypes algorithm from the kmodes library. k-modes is used for clustering categorical variables. It defines clusters based on the number of matching categories between data points. (This is in contrast to the more well-known k-means algorithm, which clusters numerical data based on Euclidean distance.) The k-prototypes algorithm combines k-modes and k-means and is able to cluster mixed numerical / categorical data (Vos 2015--2021).\n\n\n2.2.1.2 Using the Gower Distance\nGower’s Distance can be used to measure how different two records are (Gower 1971). The records may contain combination of logical, categorical, numerical or text data. The distance is always a number between 0 (identical) and 1 (maximally dissimilar). The metrics used for each data type are described below:\n\nquantitative (interval): range-normalized Manhattan distance\nordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties\nnominal: variables of k categories are first converted into k binary columns and then the Dice coefficient is used\n\nThis distance metric can be used to calculate a distance matric between all points in the dataset, which can then be used by standard hierarchical clustering. We use the scikit-learn package with its Agglomerative clustering algorithm, and cluster across multiple linkage types (as different types of linkage can produce vastly different clusters) (Pedregosa et al. 2011).\n\n\n2.2.1.3 Performance metrics\nIn order to measure how well the clustering results approximate the question category labels given, we use the Rand index for similarity. It is a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering (Rand 1971). A value of 0 indicates no similarity (clusterings do not agree on any pair of points), and 1 indicates perfect matching in clustering labels. A form of the Rand index, called the adjusted Rand index, is adjusted for the chance grouping of elements.\n\n\n\n2.2.2 Supervised Learning Models\nWe also run various supervised learning models on the data, in order to answer the following question: can non-deep learning (aka not neural network) models learn, based on the given factors in the data, whether a student will answer a question correctly? Specicifally, we run models on a transformed version of the dataset in order to predict the label column of IsCorrect. The transformations performed on the columns of the dataset are:\n\nNumerical values were min-max normalized to 0-1.\nThe timeseries column (DateAnswered) was transformed into an integer.\nThe categorical columns were one-hot encoded, where each category in each factor recieves its own column of 0-1 values (with 1 indicating that value is present), essentially creating a sparse matrix subset.\n\nWe run 7 different models on the dataset, with all implemented in scikit-learn (Pedregosa et al. 2011). The models used are as follows:\n\nLogistic Regression: A simple logistic regression classifier, where parameters (for each factor plus bias/intercept) are fitted to a linear model.\nLogistic Regression with Stochastic Gradient Descent (SGD): SGD improves on gradient descent by replacing the gradient with an esimation of it, reducing computational complexity.\nPerceptron: A linear predictor which uses a set of weights with the feature vector to output a binary clasisfier.\nLinear Support Vector Machine: Maps training examples to points in space so as to maximise the width of the gap between the two categories. Used to perform linear classification.\nDecision Tree: A model where decisions are represented by a tree/flowchart structure, each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules (Wikipedia contributors 2023).\nRandom Forest Classifier: A meta-model where a number of decision trees are fitted on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting (Pedregosa et al. 2011).\nHistogram-based Gradient Boosting Classification Tree: A meta-model that fits multiple gradient-boosted decision tree classifiers, that has support for NaN values, categorical values, and is computionally quick dues to binning inputs into histograms instead of naive evaluation.\n\n\n2.2.2.1 Performance metrics\nTo evaluate the models, we use standard performance metrics that are used for supervised learning on binary classification. The metrics we display are:\n\nAccuracy: The ratio of correct predictions to all predictions. In other words, the total of the green squares in a confusion matrix divided by the entire matrix. This is arguably the most common concept of measuring performance. It ranges from 0-1 with 1 being the best performance.\nPrecision: The ratio of true positives to the total number of positives (true positive + true negative).\nRecall: The ratio of true positives to the number of total correct predictions (true positive + false negative).\nF1 Score: Known as the harmonic mean between precision and recall. Precision and Recall are useful in their own rights, but the F1-Score is useful in the fact it’s a balanced combination of both precision and recall. It ranges from 0-1 with 1 being the best performance.\nSupport: The number of true instances for each label.\n\nIn addition, we use several visualizations to display perfomance of the models:\n\nROC Curve: A plot of the true positive rate vs the false positive rate, as a curve. We examine the AUC (area under the curve) to determine how well that a randomly chosen positive example is indeed labeled positive. If it follows the straight diagonal line, the AUC is low and therefore the classifier is no better than chance. If there’s a high AUC, then the classifier is performing well. The baseline AUC is 0.5, a a perfect classififer has 1.0\nConfusion Matrix: A matrix showing the amount of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\nPrecision-Recall Curve: A model can improve in precision or recall, but not both. A PR curve shows that tradeoff, and how well it performs in both. The curve is constructed by calculating and plotting the precision against the recall for a single classifier at a variety of thresholds. A perfect classifier would have a line that starts high and straight, and curves down only near the end of the recall axis. The summary value for the curve is the AP, or average precision; higher values towards 1 are better.\n\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "results.html#clustering",
    "href": "results.html#clustering",
    "title": "3  Results",
    "section": "3.1 Clustering",
    "text": "3.1 Clustering\nSee Figure 3.3\n\n3.1.1 Perfomance\n\n\n\n\n\n\n\n\n\n\n\n(a) Rand Index Matrix\n\n\n\n\n\n\n\n\n\n\n\n(b) Adjusted Rand Index\n\n\n\n\n\n\n\nFigure 3.3: Matrices of the rand index as compared across all clustering methods, and compared to the original source category labels. The scipy and sklearn clustering methods were done using a precomputed Gower matrix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#supervised-learning-models-performance",
    "href": "results.html#supervised-learning-models-performance",
    "title": "3  Results",
    "section": "3.2 Supervised Learning Models Performance",
    "text": "3.2 Supervised Learning Models Performance\nSee Table 3.2\n\n3.2.1 Performance\n\n\n\n\nTable 3.2: The performance values and some settings for the supervized learning models ran, sorted by accuracy.\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nprecision\nrecall\nf1-score\nsupport\nTrainIters\nLossFcn\n\n\n\n\n7\nHistGradientBoostingClassifier\n0.7144\n0.7147\n0.7144\n0.7145\n138273.0\nNaN\nlog_loss\n\n\n6\nHistGradientBoostingClassifier\n0.6602\n0.6600\n0.6602\n0.6601\n138273.0\nNaN\nlog_loss\n\n\n5\nRandomForestClassifier\n0.6519\n0.6519\n0.6519\n0.6519\n138273.0\nNaN\nNaN\n\n\n4\nDecisionTreeClassifier\n0.6108\n0.6107\n0.6108\n0.6108\n138273.0\nNaN\nNaN\n\n\n2\nLinearSVC\n0.5703\n0.5663\n0.5703\n0.5599\n138273.0\n12\nsquared_hinge\n\n\n0\nLogisticRegression\n0.5703\n0.5662\n0.5703\n0.5601\n138273.0\n[0]\nNaN\n\n\n3\nSGDClassifier\n0.5695\n0.5655\n0.5695\n0.5550\n138273.0\n41\nlog_loss\n\n\n1\nPerceptron\n0.4719\n0.4565\n0.4719\n0.4550\n138273.0\n11\nperceptron\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Performance Plots\nSee Figure 3.4\n\n\n\n\n\n\n\n\n\n\n\n(a) Perceptron: ROC Curve\n\n\n\n\n\n\n\n\n\n\n\n(b) HistGradientBoostingClassifier: ROC Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Perceptron: confusion matrix\n\n\n\n\n\n\n\n\n\n\n\n(d) HistGradientBoostingClassifier: confusion matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Perceptron: precison-recall curve\n\n\n\n\n\n\n\n\n\n\n\n(f) HistGradientBoostingClassifier: precison-recall curve\n\n\n\n\n\n\n\nFigure 3.4: The ROC, confusion matrix, and precision-recall curves for the best and worst performing model.\n\n\n\n\n\n\n\nBaek, Youngmin, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. “Character Region Awareness for Text Detection.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 9365–74.\n\n\nGhosh, Aritra, and Andrew Lan. 2021. “BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, edited by Zhi-Hua Zhou, 2410–17. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2021/332.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. http://www.jstor.org/stable/2528823.\n\n\nharadai1262. 2020. “Solution of NeurIPS Education Challenge 2020.” GitHub Repository. GitHub. https://github.com/haradai1262/NeurIPS-Education-Challenge-2020.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50. https://doi.org/10.1080/01621459.1971.10482356.\n\n\nshshen-closer. 2021. “TOP1-for-Task-2-in-the-NeurIPS-2020-Education-Challenge.” GitHub Repository. GitHub. https://github.com/shshen-closer/TOP1-for-task-2-in-the-NeurIPS-2020-Education-Challenge.\n\n\nVos, Nelis J. de. 2015--2021. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, et al. 2021. “Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge.” https://arxiv.org/abs/2104.04034.\n\n\nWang, Zichao, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel Hernández-Lobato, Richard E Turner, et al. 2020. “Instructions and Guide for Diagnostic Questions: The Neurips 2020 Education Challenge.” arXiv Preprint arXiv:2007.12061.\n\n\nWikipedia contributors. 2023. “Decision Tree — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "4  Conclusions",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#findings",
    "href": "conclusions.html#findings",
    "title": "4  Conclusions",
    "section": "4.1 Findings",
    "text": "4.1 Findings",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#limitations",
    "href": "conclusions.html#limitations",
    "title": "4  Conclusions",
    "section": "4.2 Limitations",
    "text": "4.2 Limitations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusions.html#future-studies",
    "href": "conclusions.html#future-studies",
    "title": "4  Conclusions",
    "section": "4.3 Future Studies",
    "text": "4.3 Future Studies",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  }
]