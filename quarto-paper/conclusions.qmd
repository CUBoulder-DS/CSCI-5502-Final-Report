# Conclusions

High-quality machine learning models have been of keen interest in the field of education since at least as early as the 1980s [@a2e5e87b-2409-3038-a77d-3ff68fe23bdf]. They have routinely demonstrated their potential for enhancing the quality of diagnostic testing. Given that the fundamental idea of machine learning in education is to use large amounts of student response data to monitor the current state of a students' learning and estimate the future performance of students on subsequent material, presumably after receiving a relevant course of instruction. One of the primary advantages of employing models in educational diagnostic testing is their capacity to analyze complex data sets. These models can use data from various sources, including student performance records, behavioral patterns, learning styles, and assessment results. By scrutinizing this multidimensional information, these models can discern correlations that would otherwise go unnoticed and identify subtle trends that might evade human perception [@843f6df4-f226-35d6-8e9b-45f9c3b8fb3f]. Consequently, they facilitate the early detection of learning challenges, allowing educators to intervene proactively and tailor educational strategies to suit individual student needs.

## Findings

From examining the results in @fig-rand, we can see that the clustering methods performed only adequately; if we choose to consider the adjusted Ran Index instead of the non-adjusted (as a reminder, the adjusted index accounts for the chance or random grouping of elements), we only get a maximum score of 0.29 similarity, indicating a low match in groupings with the source category labels.

From @tbl-perfs, we can see that the best performing model achieved and accuracy and F1 score of 0.71. This is an adequate model, that gives statistically better results than random chance; however, compared to deep learning models implemented on this data which achieved performance of 0.94 and greater [@NeurIPSWinner13; @NeurIPSWinner2], our models do not measure up. Nevertheless, a simplistic supervised learning model achieving such performance is a good indicator of the predictive power of the features in the input dataset of diagnostic questions. Additionally, the ROC curves and precision-recall curve of the best performing model (@fig-perf-plots) show typical and moderately well-perfoming predictive performance.

While the analysis conducted here did not produce any evidence to threaten the validity of the diagnostic test items in question and did not provide any reason to conclude that construct-irrelevant factors were driving student responses, we feel that there is reason to be optimistic about the relative success of the machine learning models. To the extent that the models were successful in generating information that could be used to guide educational decision making, they provide evidence in favor of using such models in other tasks such as machine grading, precision educational interventions, and automated tutoring. Machine grading is one area in which accurate and efficient models could ease the burden on educators by limiting the time they need to spend trying to provide meaningful and actionable feedback to students [@6b457a40-bf1a-3629-aa83-161d9245766c].

With regard to precision educational interventions, the applications of effective machine learning models span a broad spectrum of contexts. For example, existing research points to potential uses for predicting dropouts and disciplinary problems, as well as other matters related to attrition and retention [@d93ae0e3-5e88-33f9-a1a9-4d31b151e5b8]. So, even though this analysis focused on an application for predicting response patterns to mathematics questions, there is no reason that the same models could not be used for prediction and diagnosis in other areas of education. This is especially relevant in the aftermath of the massive interruptions to students' education during and after the pandemic when educators across the country are racing to find ways to remedy the severe learning loss that took place.

## Limitations

One limitation of this study is that there was relatively little student demographic information available. If a richer set of variables had been available in the data set, then it is at least conceivable that construct-irrelevance or differential item functioning might have emerged from the data. There was also no information about student response times, which is another indicator that is commonly used to investigate whether certain families of items operate in different ways among different groups of respondents. A third limitation is that very little is known about the students who are present in the sample. Fairly simple information about how much exposure students have had to certain kinds of questions and what kind of instruction they receive was unavailable. Any of these limitations on their own or in tandem could make for very different results with regard to test validity.

## Future Studies

Future iterations of this kind of work might attempt to focus the predictive abilities of the machine learning models to behavioral and social-emotional characteristics of students based on school-level data that is typically recorded. For example, attendance, chronic absences, discipline referrals, and expulsions are all kinds of information that are routinely collected by schools. This kind of information combined with standard metrics of academic performance might lead to incredibly useful insights that allow teachers and administrators to identify and design interventions for the most at-risk students.

# Link to Video Presentation and Code

- The video of the presentation can be found at [this Youtube link](https://www.youtube.com/watch?v=PPR_XuhXHT4)
- The code for this paper can be found at [this Github repo](https://github.com/CUBoulder-DS/CSCI-5502-Final-Report)


<!-- BJ Note: Leave this as is! -->
# References {.unnumbered}

::: {#refs}
:::

