# Methods

```{python}
#!pip install tabulate plotly dateutil
```

```{python}
#| include: false
import os

import pandas as pd
from IPython.display import display, Latex, Markdown

data_dir = os.path.normpath("../data/")
```

## Data

The data used is from the NeurIPS 2020 Education Challenge [@neuripsData], which is in the format of question-answer pairs of mathematical questions posed to students and their answers (and demographic). There are more than a 200 million data points in the full dataset, so we use a subset of about only a million data points. We join across the multiple tables that the data is present in, and combine them in order to have full information for each data point.

The data used format can be seen in @tbl-src-data. We dropped unused columns (`CorrectAnswer, AnswerValue, SchemeOfWorkId`), and the transformations for feature engineered columns are listed in the descriptions. The description of the columns used are as follows:

- **QuestionId**: ID of the question answered. Numeric.
- **UserId**: ID of the student who answered the question. Numeric.
- **AnswerId**: Unique identifier for the (QuestionId, UserId) pair, used to join with associated answer metadata (see below). Numeric.
- **IsCorrect**: Binary indicator for whether the student’s answer was correct (1 is correct, 0 is incorrect). Categorical.
- **SubjectId**: Each subject covers an area of mathematics, at varying degrees of gran- ularity. We provide IDs for each topic associated with a question in a list. Example topics could include “Algebra”, “Data and Statistics”, and “Geometry and Measure”. These subjects are arranged in a tree structure, so that for instance “Factorising” is the parent subject of “Factorising into a Single Bracket”. We provide details of this tree in an additional file subject metadata.csv which contains the subject name and tree level associated with each SubjectId, in addition to the SubjectId of its parent subject. Categorical.
- **Category1**: Feature engineered. The first-level category of the question (given that there is hierarchical categories). Categorical.
- **Gender**: The student’s gender, when available. 0 is unspecified, 1 is female, 2 is male and 3 is other. Categorical.
- **Age**: Feature engineered. The student's age, as calculated from `DateAnswered - DateOfBirth`. Numeric.
- **PremiumPupil**: Whether the student is eligible for free school meals or pupil premium due to being financially disadvantaged. Categorical.
- **DateAnswered**: Time and date that the question was answered, to the nearest minute. Time sequence/numeric.
- **Confidence**: Percentage confidence score given for the answer. 0 means a random guess, 100 means total confidence. Numeric.
- **GroupId**: The class (group of students) in which the student was assigned the question. Categorical.
- **QuizId**: The assigned quiz which contains the question the student answered. Categorical.

```{python}
#| label: tbl-src-data
#| tbl-cap: The source data used in this paper, tranformed by unions across several csvs, some columns dropped, and some created columns.

df_data = pd.read_csv(os.path.join(data_dir, "eedi_34_data.csv"), index_col=0)
df_data = df_data.drop(columns=["CorrectAnswer", "AnswerValue", "SchemeOfWorkId"])
display(Markdown(df_data.head().loc[:, :"GroupId"].to_markdown()))
display(Markdown(df_data.head().loc[:, "QuizId":].to_markdown()))
```

### Exploratory Analysis

We perform initial EDA, which can be seen in Section 3 (Results). The EDA performed, in order to gain insight into the data, is as follows:

- Summary statistics of each column used.
- A sunburst plot of all the subject categories found in the question dataset.
- A histogram of the proportion of questions answered correctly by each student.

## Statistical Methods Analysis

### Clustering

We perform clustering in order to answer the question: are the distances/similarity coefficients between the factors of the dataset, indicative of the subject category ID given to each data point? More specifically, when the columns given above are clustered, do we achieve a clustering similar to the labels of `Category1` given to each data point? In order to achieve this, we must take into account the categorical factors in the dataset (such as `QuizId` and `Gender`) and the fact that we cannot simply compute minimal Euclidian distances between the values, even when ordinal. We utilize 2 different methods for handling categorical factors when clustering.

#### Using the kmodes library

We use the `kprototypes` algorithm from the `kmodes` library. k-modes is used for clustering categorical variables. It defines clusters based on the number of matching categories between data points. (This is in contrast to the more well-known k-means algorithm, which clusters numerical data based on Euclidean distance.) The k-prototypes algorithm combines k-modes and k-means and is able to cluster mixed numerical / categorical data [@kmodes].

#### Using the Gower Distance

Gower’s Distance can be used to measure how different two records are [@gower]. The records may contain combination of logical, categorical, numerical or text data. The distance is always a number between 0 (identical) and 1 (maximally dissimilar). The metrics used for each data type are described below:

- quantitative (interval): range-normalized Manhattan distance
- ordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties
- nominal: variables of k categories are first converted into k binary columns and then the Dice coefficient is used

This distance metric can be used to calculate a distance matric between all points in the dataset, which can then be used by standard hierarchical clustering. We use the `scikit-learn` package with its Agglomerative clustering algorithm, and cluster across multiple linkage types (as different types of linkage can produce vastly different clusters) [@sklearn].

#### Performance metrics

In order to measure how well the clustering results approximate the question category labels given, we use the Rand index for similarity. It is a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering [@rand]. A value of 0 indicates no similarity (clusterings do not agree on any pair of points), and 1 indicates perfect matching in clustering labels. A form of the Rand index, called the adjusted Rand index, is adjusted for the chance grouping of elements.

### Supervised Learning Models

We also run various supervised learning models on the data, in order to answer the following question: can non-deep learning (aka not neural network) models learn, based on the given factors in the data, whether a student will answer a question correctly? Specicifally, we run models on a transformed version of the dataset in order to predict the label column of `IsCorrect`. The transformations performed on the columns of the dataset are:

- Numerical values were min-max normalized to 0-1.
- The timeseries column (`DateAnswered`) was transformed into an integer.
- The categorical columns were one-hot encoded, where each category in each factor recieves its own column of 0-1 values (with 1 indicating that value is present), essentially creating a sparse matrix subset.

We run 7 different models on the dataset, with all implemented in `scikit-learn` [@sklearn]. The models used are as follows:

- **Logistic Regression**: A simple logistic regression classifier, where parameters (for each factor plus bias/intercept) are fitted to a linear model.
- **Logistic Regression with Stochastic Gradient Descent (SGD)**: SGD improves on gradient descent by replacing the gradient with an esimation of it, reducing computational complexity.
- **Perceptron**: A linear predictor which uses a set of weights with the feature vector to output a binary clasisfier.
- **Linear Support Vector Machine**: Maps training examples to points in space so as to maximise the width of the gap between the two categories. Used to perform linear classification.
- **Decision Tree**: A model where decisions are represented by a tree/flowchart structure, each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules [@decisionTree].
- **Random Forest Classifier**: A meta-model where a number of decision trees are fitted on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting [@sklearn].
- **Histogram-based Gradient Boosting Classification Tree**: A meta-model that fits multiple gradient-boosted decision tree classifiers, that has support for NaN values, categorical values, and is computionally quick dues to binning inputs into histograms instead of naive evaluation.


#### Performance metrics

To evaluate the models, we use standard performance metrics that are used for supervised learning on binary classification. The metrics we display are:

- **Accuracy**: The ratio of correct predictions to all predictions. In other words, the total of the green squares in a confusion matrix divided by the entire matrix. This is arguably the most common concept of measuring performance. It ranges from 0-1 with 1 being the best performance.
- **Precision**: The ratio of true positives to the total number of positives (true positive + true negative).
- **Recall**: The ratio of true positives to the number of total correct predictions (true positive + false negative).
- **F1 Score**: Known as the harmonic mean between precision and recall. Precision and Recall are useful in their own rights, but the F1-Score is useful in the fact it's a balanced combination of both precision and recall. It ranges from 0-1 with 1 being the best performance.
- **Support**: The number of true instances for each label.

In addition, we use several visualizations to display perfomance of the models:

- **ROC Curve**: A plot of the true positive rate vs the false positive rate, as a curve. We examine the AUC (area under the curve) to determine how well that a randomly chosen positive example is indeed labeled positive. If it follows the straight diagonal line, the AUC is low and therefore the classifier is no better than chance. If there's a high AUC, then the classifier is performing well. The baseline AUC is 0.5, a a perfect classififer has 1.0
- **Confusion Matrix**: A matrix showing the amount of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).
- **Precision-Recall Curve**: A model can improve in precision or recall, but not both. A PR curve shows that tradeoff, and how well it performs in both. The curve is constructed by calculating and plotting the precision against the recall for a single classifier at a variety of thresholds. A perfect classifier would have a line that starts high and straight, and curves down only near the end of the recall axis. The summary value for the curve is the AP, or average precision; higher values towards 1 are better.

